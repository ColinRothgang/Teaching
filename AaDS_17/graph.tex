After lists, and trees, graphs are the most important data structure in computer science.
In fact, just like lists are a special of trees, trees are a special of graphs.

Data structures for lists and trees are of course used to represent lists and trees.
But they are also used to represent other data.
For example, we can represent a set as a list (Sect.~\ref{sec:ad:listset}) or as a tree (Sect.~\ref{sec:ad:redblacktree}) or a list as a tree (Sect.~\ref{sec:ad:heaplists}).
That is because choosing the more complex data structure (i.e., a tree instead of a list) can allow for more efficient algorithms.

Data structures for graphs on the other hand are almost exclusively used to represent graphs.
That is because they are rather difficult to work with.
But they are needed because graph-structured data occurs very frequently.

Practical examples of graph-structured data include
\begin{compactitem}
 \item Social networks: nodes are given by people and edges by the social connection relation.
  These relations may be symmetric (e.g., the follows-relation of twitter), which leads to directed graphs, or asymmetric (e.g., the are-friends-with-each-other relation of facebook), which leads to undirected graphs.
 \item Roads: nodes are given by cities and intersections and edges by roads between them. Flight routes, subway systems, etc. work accordingly.
 \item Utilities supply networks for water, electricity, internet, etc.: nodes are given by power plants/switchboards/hubs/etc. and households and edges by pipes/cables/etc.
 \item Mazes and other explorable territories: nodes are given by intersections and edges by paths.
 \item Games: generalizing the tree intuition from Sect.~\ref{sec:ad:minmax}, nodes are given by states and edges by steps/moves/developments.
 \item Neighborhood relations: nodes are given by countries or other territories and edges by the share-a-border-with-each-other relation.
\end{compactitem}

In fact, binary function and binary relations are the most ubiquitous complex mathematical objects.
Algorithms are our primary representation method for the former and graphs for the latter.

\section{Specification}

\begin{definition}[Graph]
A \textbf{graph} is a pair $G=(N,E)$ such that $E$ is a binary relation on $N$.

If $E$ is symmetric, $G$ is called \textbf{undirected}, otherwise \textbf{directed}.
\end{definition}

The set $N$ is usually but not necessarily finite.

Like for trees, there are many definitions to talk about the parts of a graph:

\begin{definition}[Parts of a Graph]
Consider a graph $G=(N,E)$.

An element $n\in N$ is called a \textbf{node} or a \textbf{vertex}.
An element $(m,n)\in E$ is called an \textbf{edge} from $m$ to $n$.
It is also called an \textbf{incoming edge} of $n$ and an \textbf{outgoing edge} of $m$.

For every node $n$, the number of incoming edges is called the \textbf{in-degree} of $n$, and the number of outgoing edges is called the \textbf{out-degree} of $n$.
If $G$ is undirected graph, incoming and outgoing edges are the same, and we simply speak of the \textbf{degree} of $n$.

A \textbf{path} from $a_0$ to $a_n$ is a list $[a_0,\ldots,a_n]\in N^*$ such that there is an edge from $a_{i-1}$ to $a_i$ for $i=1,\ldots,n$.

$n$ is called the \textbf{length} of the path.
If $n=0$ (and thus $a_0=a_n$), the path is called \textbf{empty}.
If there is a path from $a_0$ to $a_n$, then $a_n$ is called \textbf{reachable} from $a_0$.

A \textbf{cycle} is a non-empty path from $a$ to itself.
If $G$ has (no) cycles, it is called \textbf{(a)cyclic}.

Let us write $\ov{G}$ for the undirected graph $(N, E\cup E^{-1})$ in which all edges go both ways.
Then $G$ is called \textbf{connected} if all nodes in $\ov{G}$ are reachable from each other.

A \textbf{clique} is a subset $C$ of $N$ such that there is an edge from every $a\in C$ to every other $b\in C$.
$G$ is called \textbf{complete} if $N$ is a clique.
\end{definition}


\paragraph{Visualization}
A good intuition to think of graphs is to imagine the nodes as places and the edges as streets between them.
In a directed graph, all edges are one-way streets.

All concepts about graphs also have very intuitive visual aspects:

\begin{ctabular}{|l|l|l|}
\hline
& \multicolumn{2}{c|}{Visual Intuition}\\
Concept & undirected &  directed \\
\hline
node & \multicolumn{2}{c|}{point}  \\
edge from $a$ to $b$ & line from $a$ to $b$ & arrow from $a$ to $b$\\
incoming edge of $a$ & &  arrow pointing at $a$ \\
outgoing edge of $a$ & &  arrow pointing away from $a$ \\
$b$ reachable from $a$ & we can walk from $a$ to $b$ along edges & \ldots in arrow direction \\
path from $a$ to $b$ of length $n$ & a walk from $a$ to $b$ in $n$ steps & \ldots in arrow direction \\
{weight\footnotemark} of an edge & \multicolumn{2}{c|}{cost intuition: length of the line}\\ 
                               & \multicolumn{2}{c|}{capacity intuition: width of the line}\\ 
complete & we can walk everywhere in $1$ step &  \ldots in arrow direction \\
cycle & we can walk in a circle & \ldots in arrow direction \\
connected & \multicolumn{2}{c|}{graph can be drawn in one stroke} \\
\hline
\end{ctabular}
\footnotetext{See below for weighted graphs.}

\paragraph{Reachability Relation}
Many graph properties are just rephrasings of or closely related to relation properties.
Most importantly:

\begin{theorem}[Reachability]
For every graph $G$, the relation ``$b$ is reachable from $a$'' is
\begin{compactitem}
 \item reflexive and transitive
 \item symmetric iff $G$ is undirected
 \item anti-symmetric iff $G$ is acyclic
\end{compactitem}
\end{theorem}
\begin{proof}
Exercise.
\end{proof}

\paragraph{Labeled Graphs}
Like for trees, graphs are only useful for computation, if we can store data in them.
Contrary to trees, we often need to store data in the nodes \emph{and} the edges.

\begin{definition}[Labeled Graph]
A $A$-$B$-\textbf{labeled} graph is a triple of
\begin{compactitem}
 \item a graph $G=(N,E)$
 \item a function $nodeLabel:N\to A$
 \item a function $edgeLabel:E\to B$
\end{compactitem}
$Graph[A,B]$ is the set of $A$-$B$-labeled graphs.
\end{definition}

The most important special case arises when the nodes are not labeled (i.e., we put $A=\Unit$) and the edges are labeled with numbers, i.e., $B=\Z$:

\begin{definition}
A \textbf{weighted} graph is a $\Unit$-$B$-labeled graph where $B$ is any set of numbers.
The label of an edge is called its \textbf{weight}.
\end{definition}
Most often $B$ is $\N$, but $\Z$, $\R$, or $\Z^\infty$, etc. are also common.

There are two important applications of weighted graphs that use different interpretations of the weights:
\begin{itemize}
 \item Cost intuition: The weight of an edge is the cost of moving along the edge.
 For example, if the nodes represent cities and the edges flight routes, the weight can be the distance.
 \item Capacity intuition: The weight of an edge is the capacity for moving objects along the edge.
 For example, if the nodes represent cities and the edges flight routes, the weight can be the number of flights per day.
\end{itemize}

Correspondingly, we define:
\begin{definition}
Consider a weighted graph.
We write $weight(i,j)$ for the weight of the edge from $i$ to $j$.

We make $weight:N\times N\to \N^\infty$ a total function by using a default value whenever there is no edge from $i$ to $j$:
\begin{compactitem}
 \item A \textbf{cost-weighted} graph uses the default $weight(i,j)=\infty$.
 \item A \textbf{capacity-weighted} graph uses the default $weight(i,j)=0$.
\end{compactitem}

In a cost-weighted graph, the \textbf{cost of a path} is the sum of the weights of all edges.

In a capacity-weighted graph, the \textbf{capacity of a path} is the minimal weight of any edge in it.
\end{definition}

The intuition behind the default values is that if there is no edge from $i$ to $j$,
\begin{compactitem}
 \item cost intuition: it is impossible to go from $i$ to $j$, i.e., which corresponds to infinite cost,
 \item capacity intuition: it is impossible to move objects from $i$ to $j$, which corresponds to empty capacity.
\end{compactitem}

\section{Data Structures}

We consider only graphs $G=(N,E)$ where $|N|=m$, i.e., is finite.
We number the elements of $N$ (in any order) so that we can assume $N=\Z_m$.

Moreover, we consider only graphs whose edges are labeled with weights from set $B$.
Note that an unlabeled graph can be seen as the special case where $B=\Unit$.

Graphs are among the trickiest data structures to design because it is difficult to represent the set $E$ efficiently.
There are a number of frequently-needed operations, whose complexity depends on the data structure.
We may want to obtain
\begin{compactitem}
 \item $edge(G,i,j)$: for two nodes $i$, $j$, the edge from $i$ to $j$ (if any),
 \item $outgoing(G,i)$: for a node $i$, the list of outgoing edges,
 \item $incoming(G,i)$: for a node $i$, the list of incoming edges,
 \item $edges(G)$: an iterator over all edges.
\end{compactitem}

If we have one of those operations, we can compute the others---but not necessarily efficiently.

\subsection{Adjacency Matrix}

\paragraph{Design}
An often useful representation is via a matrix, called the \textbf{adjacency matrix} of $G$.
It stores the entire function that maps two nodes to their edge in a single object.

\begin{definition}[Adjacency Matrix]
The adjacency matrix of an unlabeled graph $G$ is the matrix $Adj\in \Bool^{mm}$ where $Adj_{ij}==\true$ iff there is an edge from $i$ to $j$ in $G$.
\end{definition}

Adjacency matrices have the nice property that we can multiply them.
Here matrix multiplication is computed using conjunction and disjunction instead of multiplication and addition:
\begin{definition}
For $X,Y\in \Bool^{mm}$, we define $(X\cdot Y)_{ik}:= \bigvee_{j=0,\ldots,m-1}X_{ij}\wedge Y_{jk}$.
\end{definition}
This is useful because:
\begin{theorem}
If $Adj$ is the adjacency matrix of $G$, then $(Adj^n)_{ij}$ iff there is a path of length $n$ from $i$ to $j$ in $G$.
\end{theorem}

This is advantageous because it lets us compute all paths of length $n$ efficiently by computing $Adj^n$, e.g., using square-and-multiply (Sect.~\ref{sec:ad:exp:sqmult}).
Moreover, in an acyclic graph, there are only finitely many paths.
Thus, we eventually have $Adj^n=Adj^{n+1}=\ldots$, at which point we have computed all paths.

If the edges are labeled with weights from $B$, we can use an adjacency matrix $W\in B^{mm}$ such that $W_{ij}$ is the weight of the edge from $i$ to $j$.
If there is no edge, we use a default value, e.g., $\infty$, $0$, or $null$.

\paragraph{Complexity}
A drawback of the adjacency matrix is that its size $|N|^2$.
Moreover, for an undirected graph, half the space is wasted because we always have $Adj_{ij}=Adj_{ji}$.

$edge(G,i,j)$ takes $\Theta(1)$ (assuming we store the matrix efficiently using arrays).

$outgoing(G,i)$ and $incoming(G,i)$ must pull out a row or column from the adjacency matrix.
That takes $\Theta(1)$ or $\Theta(m)$, depending on how we store the arrays.

$edges(G)$ takes $\Theta(1)$ because we can use the iterator of the array.

\subsection{Adjacency Lists}

\paragraph{Design}
For graphs with many nodes and few edges, it is better to store adjacency lists.
Those are the lists of outgoing edges for each node:

\begin{definition}[Adjacency List]
For an unlabeled graph, the adjacency list of a node $i$ is the sorted list $L_i\in List[\Z_m]$ of all $j$ such that there is an edge from $i$ to $j$ in $G$.

The adjacency list--representation of $G$ consists of an list $[(0,L_0),\ldots,(m-1,L_{m-1})]$ pairing every node with its adjacency list.
\end{definition}

If the edges are labeled with weights from $A$, the adjacency list $L_i\in List[\Z_m\times A]$ contains pairs $(j,w)$ where $W$ is the weight of the edge from $i$ to $j$.
We do not need default values because we can simply omit those $j$ for which there is no edge.

\paragraph{Complexity}
The size of the adjacency list--representation is $|N|+|E|$, which is usually much smaller than $|N|^2$.

$edge(G,i,j)$ takes $\Theta(d)$ where $d$ is the maximal number of outgoing edges of any node.
$d$ is usually much smaller than $|E|$.

$outgoing(G,i)$ takes $\Theta(1)$.

$incoming(G,i)$ is inefficient. We have to check each node for an edge into $i$ and collect the results.

$edges(G)$ takes $\Theta(m)$ because we have to concatenate $m$ iterators.

\subsection{Opposite Adjacency List}

\paragraph{Design}
The adjacency list stores for every node the list of \emph{outgoing} edges.

For directed graphs, we can alternatively store the list of incoming edges.
That is the same as storing the adjacency lists for the graph in which all edges are flipped.

\paragraph{Complexity}
All results are opposite to the previous case.

\subsection{Redundant Adjacency Lists}

\paragraph{Design}

If we want to be able to do both, it is best to store both adjacency lists for each node.

\paragraph{Complexity}
The size of the representation is now twice as big as when using a single adjacency list.

$edge(G,i,j)$ takes $\Theta(d)$ as before.

$outgoing(G,i)$ and $incoming(G,i)$ take $\Theta(1)$.

$edges(G)$ takes $\Theta(m)$ because we have to concatenate $m$ iterators.

The main drawback of this data structure is that it is more difficult to implement because we have to keep the adjacency lists in sync.
Every time we add or remove an edge from $i$ to $j$, we have to change two lists, namely $incoming(G,j)$ and $outgoing(G,i)$.

\section{Important Algorithms}

Unless mentioned otherwise, we use a connected finite directed graph $G=(N,E)$, whose edges are labeled with natural numbers.
We assume $N=\Z_m$.

\subsection{Reachability}\label{sec:ad:reachability}

We work with unlabeled graphs in this section.

\paragraph{Problem}
Given a start node $n\in N$, our goal is to explore all nodes that are reachable from $n$.

This problem of exploring all reachable nodes comes up all the time in practice.
In a maze, it is a way to find the exit.
In a game, it means to analyze a particular situation in the game.

\paragraph{Algorithm}
We use a data structure for which $outgoing$ is efficient.
Then we start at $n$ are recursively call $outgoing$ until we reach no further nodes.

Like for trees, there are two typical options: DFS and BFS.
Contrary to trees, we have to watch for cycles: if $G$ has a cycle and we do not keep track of which nodes we have found already, we would never terminate.
Therefore, we use two auxiliary data structures:
\begin{compactitem}
 \item a set $reachable:List[N]$ where we store all nodes in the order in which we found them and which we return eventually
 \item a queue $horizon:List[N]$ where we store all nodes that we have found already but whose outgoing edges we have not yet looked at
\end{compactitem}

For BFS, the algorithm looks as follows:

\begin{acode}
\afun[{List[N]}]{BFS}{G:Graph,start:N}{
	reachable: List[N] = Nil\\
	horizon: Queue[N] = \anew{Queue[N]}{}\\
	enqueue(horizon, start)\\
	\awhile{!empty(horizon)}{
	  i = dequeue(horizon)\\
	  reachable = prepend(i, reachable)\\
	  foreach(outgoing(G,i), \alam{j}{\ablock{
	    \aif[the critical check to avoid cycles]{!contains(reachable,j) \wedge !contains(horizon,j)}{
	      enqueue(horizon,j)
	    }
	  }}\\)
	}\\
	reverse(reachable)
}
\end{acode}

Like for trees, we obtain a DFS exploration algorithm if we use a stack instead of a queue.

\paragraph{Correctness}
The postcondition is that $BFS$ return the nodes reachable from $start$.
As a loop invariant we use that
\begin{compactitem}
 \item every node reachable from $start$ or from a node in $reachable$ is
		\begin{compactitem}
		 \item an element of $reachable$ or
		 \item reachable from some element of $horizon$,
		\end{compactitem}
	and
 \item all nodes in $reachable$ or $horizon$ are reachable from $start$.
\end{compactitem}

As a termination ordering we use $|N|-length(reachable)$.
Indeed, reachable grows in every iteration of the loop and (because it never contains duplicates) becomes at most $|N|$.

\paragraph{Complexity}
Let $r<|N|$ be the number of reachable nodes and $e$ be the number of edges between reachable nodes.
Then $BFS$ takes $\Theta(r+e)$ because every reachable node is visited once and every edge is traversed once.

\subsection{Minimal Spanning Tree}\label{sec:ad:spanningtree}

We work with undirected graphs in this section.

\paragraph{Problem}
A \textbf{spanning tree} for a graph $G$ is a subgraph of $G$ that is a tree and includes all nodes of $G$.
In other words, it is a subset of edges of $G$ such that $G$ becomes a tree.

Spanning trees are closely related to the exploration of reachable nodes: they provide a minimal set of edges needed to reach all reachable nodes.
For example, when planning to roads or laying cables a spanning tree can provide the minimal amount of connections needed to reach all households.

We can capture the minimality condition with the following observations.
A spanning tree has $|N|-1$ edges because every node but the root has exactly one incoming edge.
It is not possible to use fewer edges because any connected subgraph of $G$ that includes all nodes must have at least $|N|-1$ edges: $1$ edge can connect two nodes; any additional edge can connect only one additional node.

The problem becomes more interesting if the edges of $G$ are cost-weighted.
Then the \textbf{weight of a subgraph} $T$ of $G$ is the sum of the weights of the edges in $T$.
Then our goal is not only to find \emph{some} spanning tree but a minimal one, i.e., a spanning tree with the least possible weight.

For example, when planning roads, the cost of an edge could be the distance (more generally: the financial cost of building the road) between two points.
A minimal spanning tree minimizes the overall cost of building the roads.

\paragraph{Algorithm}
The BFS and DFS exploration algorithms immediately yield algorithms to find \emph{some} spanning tree.
All we have to do is to store the nodes we find in a $reachable: Tree[N]$ instead of $reachable:List[N]$.

It is less obvious how to find a minimal spanning tree of a cost-weighted graph.
One of the most well-known solutions is Kruskal's algorithm:

\begin{acode}
\afun[{Array[N]}]{Kruskal}{G:Graph}{
  edges := \text{list of edges $e\in E$, sorted by increasing weight}\\
  sot := \anew{Set[E]}{}\\
  foreach(edges, \alam{e}{\aifI{isSetOfTrees(sot,\cup\{e\})}{insert(tree,e)}})\\
  sot
}
\end{acode}

Here the function $isSetOfTrees$ has the following postcondition: if $S\sq E$ is a set of edges of $G$, then $isSetOfTrees(S)$ is true if the subgraph of $G$ containing only the edges in $S$ is a set of trees.\footnote{Such a graph is called a \textbf{forest}.}

\paragraph{Correctness and Complexity}
This is an example of a greedy algorithm.
We discuss its correctness and complexity in Ch.~\ref{sec:ad:greedy}.

\subsection{Shortest Path}\label{sec:ad:shortestpath}

In this section, we interpret all edge weights as costs.
We think of the cost as the \emph{distance} between nodes.

\paragraph{Problem}
Recall that the cost of a path is the sum of the costs of its edges.
We can think of it as the \emph{length} of the path.

Our goal is to find, for nodes $i$, $j$, the shortest path from $i$ to $j$.
This has obvious applications in logistics, navigation, and similar situations.
Whenever we move in a network, we prefer taking the shortest path.

\paragraph{Algorithms}
Forthcoming.
% single-source: Dijkstra, greedy
% all-pairs: matrix powers for min-+ or Floyd Warshall, dynamic programming

\subsection{Maximal Flow}\label{sec:ad:maximalflow}

In this section, we interpret all edge weights as capacities.
We assume that $G=(N,E)$ has two distinguished nodes $source$ with only outgoing and $sink$ with only incoming edges.
Such a graph is called a \textbf{flow network}.

Intuitively, we think of the edges as pipes, and our goal is to make a liquid flow from the source to the sink.

\paragraph{Flows}
A flow is a mapping $f:E\to \N$ such that the conditions below hold.

Intuitively, a flow says how much is flowing along an edge.
To make sense, a flow must satisfy the following conditions:
\begin{compactitem}
 \item No edge is used beyond capacity: $f(e)\leq w(e)$ for all $e\in E$.
 \item Nothing gets lost or created along the way, i.e., as much flows into a node as flows out: for all nodes $n$ except for $source$ and $sink$
  \[\Sigma_{e\in incoming(n)} f(e) = \Sigma_{e\in outgoing(n)} f(e)\]
\end{compactitem}

From that we can prove that
 \[\Sigma_{e\in outgoing(source)} f(e) = \Sigma_{e\in incoming(sink)} f(e)\]
and that number is called the capacity $Cap(f)$ of $f$.
It describes how much is flowing through the network from the source to the sink.

\paragraph{Problem}
Now our goal is to find $f$ such $Cap(f)$ is maximal.

For example, the flow network describes goods that have to be shipped from one place to another via a variety of paths, the maximal flow maximizes the amount of goods that are shipped.
Similar examples abound in logistics.

\paragraph{Algorithm}
Forthcoming.
%It is straightforward to find the maximal flow once we understand greedy algorithms.
%We get back to it in Ch.~\ref{sec:ad:greedy}.